{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a LangChain AI Agent in Python with watsonx \n",
    "\n",
    "**Author**: Anna Gutowska \n",
    "\n",
    "In this tutorial, we will use the LangChain Python package to build an AI agent that utilizes its custom tools to return a URL directing to [NASA's Astronomy Picture of the Day](https://apod.nasa.gov/apod/astropix.html). \n",
    "\n",
    "An [artificial intelligence (AI) agent](https://www.ibm.com/think/topics/ai-agents), otherwise known as a [Large Language Model (LLM)](https://www.ibm.com/topics/large-language-models) agent, is a system that performs tasks on behalf of a user or another system by designing its own workflow and utilizing available tools.\n",
    "\n",
    "# Overview of AI agents \n",
    "One of the most common modalities of agentic AI approaches are chatbots. However, agentic technology can encompass a wide range of functions. These include planning, problem-solving, interacting with external environments, and executing actions. These agents can be deployed to solve complex tasks in various enterprise contexts. From software design and IT automation, including customer service bots, to code-generation tools and conversational AI assistants, AI agents leverage the capability of LLMs to work step-by-step.\n",
    "\n",
    "Key processes that make AI agents unique in their autonomy are: \n",
    "\n",
    "* **Goal initializaiton and planning**. Although AI agents are autonomous in their planning of future actions, they require goals and environments defined by humans.\n",
    "\n",
    "* **Reasoning using available tools**. An AI agentâ€™s plan of action is based on the information it perceives, or its percepts. Often, AI agents do not have the full knowledge base needed for tackling all subtasks within a complex goal. To remedy this, AI agents use their available tools. These tools can include external datasets, algorithms, search tools, APIs and even other agents. We can give agents instructions to first \"think\" and plan their response before deciding which tools to use and iteratively improving the response, if needed. This can be done by aligning the predefined prompt structure with a ReAct (Reasoning and Action) framework. The ReAct prompting technique essentially tells the agent to reason slowly and to display each \"thought\". The verbal reasoning produced gives insight into how responses are being formulated. On the contrary, planning is not a necessary step for an agent when working on simple tasks. Instead, an agent can iteratively reflect on its responses and improve them without planning its next steps.\n",
    "\n",
    "* **Learning and reflection**. AI agents use feedback mechanisms, such as other AI agents and human-in-the-loop (HITL), to improve the accuracy of their responses. \n",
    "\n",
    " Traditional LLMs, like the IBM [Granite Model](https://www.ibm.com/products/watsonx-ai/foundation-models) we will be using in this tutorial, produce their responses based on the data used to train them. There is a knowledge and reasoning limitation. This means that current information, such as today's date, is not available to the model without any additional tools. In contrast, agentic technology utilizes tool calling on the backend to obtain up-to-date information, optimize workflow, and create specific tasks autonomously to achieve complex goals. In this process, the autonomous agent is learning to adapt to user expectations over time, providing a  personalized experience and comprehensive responses. This tool calling can be achieved without human intervention and broadens the possibilities for real-world applications of these AI systems.\n",
    "\n",
    "We encourage you to check out our [AI Agents article](https://www.ibm.com/think/topics/ai-agents) for more in-depth information on the various AI agent architectures and their abstractions. \n",
    "\n",
    "# Prerequisites\n",
    "\n",
    "You need an [IBM Cloud account](https://cloud.ibm.com/registration?utm_source=ibm_developer&utm_content=in_content_link&utm_id=tutorials_awb-create-langchain-rag-system-python-watsonx&cm_sp=ibmdev-_-developer-_-trial) to create a [watsonx.ai](https://www.ibm.com/products/watsonx-ai?utm_source=ibm_developer&utm_content=in_content_link&utm_id=tutorials_awb-create-langchain-rag-system-python-watsonx&cm_sp=ibmdev-_-developer-_-product) project.\n",
    "\n",
    "# Steps\n",
    "\n",
    "## Step 1. Set up your environment\n",
    "\n",
    "While you can choose from several tools, this tutorial walks you through how to set up an IBM account to use a Jupyter Notebook. Jupyter Notebooks are widely used within [data science](https://www.ibm.com/topics/data-science?utm_source=ibm_developer&utm_content=in_content_link&utm_id=tutorials_awb-create-langchain-rag-system-python-watsonx&cm_sp=ibmdev-_-developer-tutorials-_-ibmcom) to combine code, text, images, and [data visualizations](https://www.ibm.com/topics/data-visualization?utm_source=ibm_developer&utm_content=in_content_link&utm_id=tutorials_awb-create-langchain-rag-system-python-watsonx&cm_sp=ibmdev-_-developer-tutorials-_-ibmcom) to formulate a well-formed analysis.\n",
    "\n",
    "1. Log in to [watsonx.ai](https://dataplatform.cloud.ibm.com/registration/stepone?utm_source=ibm_developer&utm_content=in_content_link&utm_id=tutorials_awb-create-langchain-rag-system-python-watsonx&cm_sp=ibmdev-_-developer-_-trial) using your IBM Cloud account.\n",
    "\n",
    "2. Create a [watsonx.ai project](https://www.ibm.com/docs/en/watsonx/saas?utm_source=ibm_developer&utm_content=in_content_link&utm_id=tutorials_awb-create-langchain-rag-system-python-watsonx&topic=projects-creating-project&cm_sp=ibmdev-_-developer-tutorials-_-ibmcom).\n",
    "\t- Note the project ID in project > Manage > General > Project ID. You'll need this for the tutorial.\n",
    "\n",
    "3. Create a [Jupyter Notebook](https://www.ibm.com/docs/en/watsonx/saas?utm_source=ibm_developer&utm_content=in_content_link&utm_id=tutorials_awb-create-langchain-rag-system-python-watsonx&topic=editor-creating-managing-notebooks&cm_sp=ibmdev-_-developer-tutorials-_-ibmcom).\n",
    "\n",
    "This step will open a Notebook environment where you can copy the code from this tutorial to implement an AI agent of your own. Alternatively, you can download this notebook to your local system and upload it to your watsonx.ai project as an asset. This Jupyter Notebook is available on GitHub.\n",
    "\n",
    "## Step 2. Set up a Watson Machine Learning service instance and API key\n",
    "\n",
    "1. Create a [Watson Machine Learning](https://cloud.ibm.com/catalog/services/watson-machine-learning?utm_source=ibm_developer&utm_content=in_content_link&utm_id=tutorials_awb-create-langchain-rag-system-python-watsonx&cm_sp=ibmdev-_-developer-tutorials-_-trial) service instance (choose the Lite plan, which is a free instance).\n",
    "\n",
    "2. Generate an [API Key in WML](https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/ml-authentication.html?utm_source=ibm_developer&utm_content=in_content_link&utm_id=tutorials_awb-create-langchain-rag-system-python-watsonx&context=cpdaas). \n",
    "\t- Can be found in Manage > Access (IAM) > API Keys (left hand panel) > Create +. Save this API key for use in this tutorial.\n",
    "\n",
    "\n",
    "3. Associate the WML service to the project you created in [watsonx.ai](https://dataplatform.cloud.ibm.com/docs/content/wsj/getting-started/assoc-services.html?utm_source=ibm_developer&utm_content=in_content_link&utm_id=tutorials_awb-create-langchain-rag-system-python-watsonx&context=cpdaas). \n",
    "\t- Can be found in Project > Services & Integrations > + Associate Service > Add \"Watson Machine Learning\"\n",
    "\n",
    "## Step 3. Install and import relevant libraries and set up your credentials\n",
    "\n",
    "We'll need a few libraries and modules for this tutorial. Make sure to import the ones below, and if they're not installed, you can resolve this with a quick pip install. LangChain will be the framework and developer toolkit used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#installations\n",
    "%pip install pandas\n",
    "%pip install langchain\n",
    "%pip install langchain_ibm\n",
    "%pip install langchain_core\n",
    "%pip install IPython\n",
    "%pip install nasapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import nasapy\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.tools import tool\n",
    "from langchain.agents import AgentExecutor\n",
    "from langchain.agents.format_scratchpad import format_log_to_str\n",
    "from langchain.agents.output_parsers import JSONAgentOutputParser\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.tools.render import render_text_description_and_args\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_ibm import WatsonxLLM\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up your credentials. Input your API key and project ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "credentials = {\n",
    "    \"url\": \"https://us-south.ml.cloud.ibm.com\",\n",
    "    \"apikey\": \"YOUR_API_KEY_HERE\",   \n",
    "    \"project_id\": \"YOUR_PROJECT_ID_HERE\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's establish our connection with the NASA API that we will be using later in this activity. This API does not require authentication. With a small rate limit, you can get started using 'DEMO_KEY' as your key. To avoid the low rate limitations, you can [register for your own NASA API key](](https://api.nasa.gov/)) and replace it as the key value below. Registering for a personal key is quick, free, and simple. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = nasapy.Nasa(key='DEMO_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4. Initialize a basic agent with no tools\n",
    "\n",
    "This step is important as it will produce a clear example of an agent's behavior with and without tools. Let's start by setting our parameters. \n",
    "\n",
    "The model parameters available can be found [here](https://ibm.github.io/watson-machine-learning-sdk/model.html). We experimented with various model parameters, including Temperature, Top P, and Top K. [Here's](https://www.ibm.com/docs/en/watsonx/saas?topic=lab-model-parameters-prompting) some more information on model parameters and what they mean. It is important to set our `stop_sequences` here in order to limit agent hallucination. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    \"decoding_method\": \"greedy\",\n",
    "    \"temperature\": 0, \n",
    "    \"min_new_tokens\": 5,\n",
    "    \"max_new_tokens\": 250,\n",
    "    \"stop_sequences\":['\\nObservation', '\\n\\n']\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this tutorial, we suggest using IBM's Granite 13B Chat model as the LLM to achieve similar results. You are free to use any AI model of choice. The purpose of these models in LLM applications is to serve as the reasoning engine that decides which actions to take."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = WatsonxLLM(\n",
    "\tmodel_id =  \"ibm/granite-13b-chat-v2\",\n",
    "\turl = credentials.get(\"url\"),\n",
    "\tapikey = credentials.get(\"apikey\"),\n",
    "\tproject_id =  credentials.get(\"project_id\"),\n",
    "\tparams = param\n",
    "\t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll set up a prompt template in case you would like to ask multiple questions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"Answer the {query} accurately.\"\n",
    "prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can set up a chain with our prompt and our LLM. This allows the generative model to produce a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = prompt | model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step of this tutorial, we will be creating a tool that retrieves today's date. As we have covered, traditional LLMs cannot obtain the current date on their own. Let's verify this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nA: Today is [display current date].\\n\\n'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.invoke({\"query\": \"What is today's date?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evidently, the LLM is unable to provide us with the current date. The training data used for this model contained information prior to today's date and without the appropriate tools, the agent does not have access to real-time information. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5. Define the agent's tools\n",
    "\n",
    "Unlike traditional LLMs, AI agents can provide more comprehensive responses to diverse tasks through their tool usage, memory, and planning. Let's define the two tools our agent will be using: \n",
    "\n",
    "* `get_todays_date()` - uses the Python `datetime` package to return today's date in YYYY-MM-DD format. \n",
    "* `get_astronomy_image()` - utilizes the NASA API to obtain the Astronomy Picture of the Day. After the tool acquires the image, its URL is returned. \n",
    "\n",
    "Note: The date variable is set to an empty string because it is used by both tools and is thus, a global variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = ''   \n",
    "\n",
    "@tool\n",
    "def get_todays_date():\n",
    "    \"\"\"Get today's date in YYYY-MM-DD format.\"\"\"\n",
    "    global date\n",
    "    date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    return date\n",
    "\n",
    "\n",
    "@tool(return_direct=True)\n",
    "def get_astronomy_image():\n",
    "    \"\"\"Get NASA's Astronomy Picture of the Day on given date.\"\"\"\n",
    "    global date\n",
    "        \n",
    "    apod = n.picture_of_the_day(date, hd=True)\n",
    "    \n",
    "    return apod['url']\n",
    "\n",
    "\n",
    "tools = [get_todays_date, get_astronomy_image]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6. Generate a response with the AI agent\n",
    "\n",
    "Next, we will set up a prompt template to ask multiple questions. This template tells the agent how to print its \"thought process\". This \"thought process\" refers to a view of the agent's subtasks, tools used, and final output. This prompt also instructs the agent to return its responses in JSON Blob format and to consider information it has stored in its memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"Respond to the human as helpfully and accurately as possible. You have access to the following tools:\n",
    "{tools}\n",
    "Use a json blob to specify a tool by providing an action key (tool name) and an action_input key (tool input).\n",
    "Valid \"action\" values: \"Final Answer\" or {tool_names}\n",
    "Provide only ONE action per $JSON_BLOB, as shown:\"\n",
    "```\n",
    "{{\n",
    "  \"action\": $TOOL_NAME,\n",
    "  \"action_input\": $INPUT\n",
    "}}\n",
    "```\n",
    "Follow this format:\n",
    "Question: input question to answer\n",
    "Thought: consider previous and subsequent steps\n",
    "Action:\n",
    "```\n",
    "$JSON_BLOB\n",
    "```\n",
    "Observation: action result\n",
    "... (repeat Thought/Action/Observation N times)\n",
    "Thought: I know what to respond\n",
    "Action:\n",
    "```\n",
    "{{\n",
    "  \"action\": \"Final Answer\",\n",
    "  \"action_input\": \"Final response to human\"\n",
    "}}\n",
    "Begin! Reminder to ALWAYS respond with a valid json blob of a single action.\n",
    "Respond directly if appropriate. Format is Action:```$JSON_BLOB```then Observation\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we are telling the agent to display the intermediate steps it takes in the `agent_scratchpad` following the user input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_prompt = \"\"\"{input}\n",
    "{agent_scratchpad}\n",
    "(reminder to always respond in a JSON blob)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we establish the order of human, agent, and tool messages. We create the template to feature the previously defined `system_prompt` followed by an optional list of messages collected in the agent's memory, if any, and finally, the `human_prompt` which includes both the `input` and `agent_scratchpad`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\", optional=True),\n",
    "        (\"human\", human_prompt),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets finalize our prompt template by rendering the tool names, descriptions, and arguments in plain text. This allows the agent to access the information pertaining to each tool and its corresponding intended use case. This also means we can add and remove tools without needing to alter our entire prompt template. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = prompt.partial(\n",
    "    tools=render_text_description_and_args(list(tools)),\n",
    "    tool_names=\", \".join([t.name for t in tools]),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important feature of AI agents is their memory. Agents are able to store past conversations and past findings in their memory to improve the accuracy and relevance of their responses going forward. In our case, we will use LangChain's `ConversationBufferMemory()` as a means of memory storage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can set up a chain with our agent's scratchpad, memory, our prompt and our LLM. The `AgentExecutor` class is used to execute the agent. It takes the agent, its tools, error handling approach, verbose parameter, and memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = ( RunnablePassthrough.assign(\n",
    "        agent_scratchpad=lambda x: format_log_to_str(x[\"intermediate_steps\"]),\n",
    "        chat_history=lambda x: memory.chat_memory.messages,\n",
    "    )\n",
    "    | prompt | model | JSONAgentOutputParser()                                               \n",
    ")                           \n",
    "\n",
    "agent_executor = AgentExecutor(agent=chain, tools=tools, handle_parsing_errors=True, verbose=True, memory=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now able to ask the agent questions. Recall the agent's previous inability to provide us with the current date. Now that the agent has its tools available to use, let's try asking the same question again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"get_todays_date\",\n",
      "  \"action_input\": {}\n",
      "}\n",
      "```\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m2024-07-09\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Final Answer\",\n",
      "  \"action_input\": \"Today's date is 2024-07-09.\"\n",
      "}\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': \"What is today's date?\",\n",
       " 'history': '',\n",
       " 'output': \"Today's date is 2024-07-09.\"}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\": \"What is today's date?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! The agent is now able to tell us the current date. As seen in the JSON Blob produced, the agent recognized that it should utilize the `get_todays_date` tool to reach this result. \n",
    "\n",
    "Now, let's try asking a more complex question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"get_todays_date\",\n",
      "  \"action_input\": {}\n",
      "}\n",
      "```\n",
      "Observation\u001b[0m\u001b[36;1m\u001b[1;3m2024-07-09\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"get_astronomy_image\",\n",
      "  \"action_input\": {}\n",
      "}\n",
      "```\n",
      "Observation\u001b[0m\u001b[33;1m\u001b[1;3mhttps://apod.nasa.gov/apod/image/2407/NoctilucentFlorida_Pouquet_960.jpg\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': \"Show me NASA's Astronomy Picture of the Day today.\",\n",
       " 'history': \"Human: What is today's date?\\nAI: Today's date is 2024-07-09.\",\n",
       " 'output': 'https://apod.nasa.gov/apod/image/2407/NoctilucentFlorida_Pouquet_960.jpg'}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\": \"Show me NASA's Astronomy Picture of the Day today.\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neat! The agent used both of its available tools to return a URL which leads to an image of today's Astronomy Picture of the Day via NASA's API. Check out the image [here](https://apod.nasa.gov/apod/image/2407/GianniTumino_Etna&MW_14mm_JPG_LOGO__1024pix.jpg \"here\"). Since we wanted to see the image  from today, the agent used the `get_todays_date` tool first in order to then use that date for the `get_astronomy_image` tool. Additionally, the agent is successfully updating its knowledge base as it learns new information and experiences new interactions as seen by the history output. \n",
    "\n",
    "And that's it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary and next steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, you created a AI agent using LangChain in Python with watsonx. You created a tool to return today's date and another tool to return today's Astronomy Picture of the Day using NASA's open-source API.\n",
    "\n",
    "The sample output is important as it shows the steps the agent took in creating its own agent workflow using available tools. In our case, the LLM on its own was not able to achieve the first subtask of the problem - finding the current date. Hence, the tools granted to the agent were incredibly useful and vital for achieving the goal. \n",
    "\n",
    "We encourage you to check out the [LangChain documentation page](https://python.langchain.com/v0.2/docs/tutorials/agents/) for more information and tutorials on AI agents. \n",
    "\n",
    "\n",
    "## Try wantsonx for free\n",
    "\n",
    "Build an AI strategy for your business on one collaborative AI and data platform called IBM [watsonx](https://www.ibm.com/watsonx?utm_source=ibm_developer&utm_content=in_content_link&utm_id=tutorials_awb-create-langchain-rag-system-python-watsonx&cm_sp=ibmdev-_-developer-_-product), which brings together new generative AI capabilities, powered by foundation models, and traditional machine learning into a powerful platform spanning the AI lifecycle. With [watsonx.ai](https://www.ibm.com/products/watsonx-ai?utm_source=ibm_developer&utm_content=in_content_link&utm_id=tutorials_awb-create-langchain-rag-system-python-watsonx&cm_sp=ibmdev-_-developer-_-product), you can train, validate, tune, and deploy models with ease and build AI applications in a fraction of the time with a fraction of the data.\n",
    "\n",
    "Try [watsonx.ai](https://dataplatform.cloud.ibm.com/registration/stepone?utm_source=ibm_developer&utm_content=in_content_link&utm_id=tutorials_awb-create-langchain-rag-system-python-watsonx&cm_sp=ibmdev-_-developer-_-trial), the next-generation studio for AI builders.\n",
    "\n",
    "## Next steps\n",
    "\n",
    "Explore [moreâ€¯articles and tutorials about watsonx](https://developer.ibm.com/components/watsonx/?utm_source=ibm_developer&utm_content=in_content_link&utm_id=tutorials_awb-create-langchain-rag-system-python-watsonx)â€¯on IBM Developer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
